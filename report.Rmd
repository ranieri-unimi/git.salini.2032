# laddering unite

```{r}
library(scales)
```


Two datasests have been used.

## pokemon features
The first dataset carries features of each pokemon. Raw data are stored by pokemon level, let's merge them:

### data ingestion
```{r}
read_stats = function(){
  df = data.frame()
  for(i in 1:15){
    uri = sprintf("datasets/stats/%s.csv", i)
    dfi = read.csv(uri)
    dfi$level = i
    df = rbind(df, dfi)
  }
  return(df)
}
```

```{r}
df = read_stats()
```

(names for visualization purpose)

```{r}
df_rn = paste(
  toupper(substr(df$name, 1,4)),
  df$level,
  sep="")

row.names(df) = df_rn
df$name = NULL
```

```{r}
df$speed = NULL
```

Each pokemon is described by 10 features. Let's normalize them:

```{r}
for(i in 1:length(df)){
  df[i] = ((df[i] - min(df[i])) /(max(df[i])-min(df[i])))
}

df = df[df$level > (13/15),]
df$level = NULL
```

## dimensionality reduction
pokemon features are a bit redundant, let's reduce dimensions with pca!

```{r}
res = princomp(df, cor=T)
summary(res)
```

```{r}
res$loading
```

```{r}
screeplot(res)
plot(res$scores, cex=0.0)
text(res$scores, rownames(df), cex=0.6)
abline(h=0, v=0)
```

```{r}
pr.var=res$sdev^2
pve=pr.var/sum(pr.var)

plot(cumsum(pve), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", ylim=c(0,1),type='b')
```

## clustering
Instead of embedding each pokemon with its coordinates on the space of the tw pricipal components, k means has been applied to discover groups od interchangeable Pokemon.

We cannot use k means over the original point cloud, because features are not comparable (distances between points are not euclidean, and an appropriate distance has not been found)

```{r}
biplot(res, cex=0.5)
abline(h=0, v=0)
```

```{r}
wssplot <- function(data, nc=15, seed=1234){
  wss = (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")}

wssplot(df[2:length(df)], nc=12) 
```

```{r}
library(cluster)

clusplot(df, kmeans(df, centers=6)$cluster, 
         main='2D representation of the Cluster solution',
         color=TRUE, shade=TRUE,
         labels=2, lines=0, cex=0.5)
```

hack: a better clustering has been found using DB SCAN

```{r}
# Compute DBSCAN using fpc package
library("fpc")

db = fpc::dbscan(res$scores, eps = 1.6, MinPts = 2)

# Plot DBSCAN results
library("factoextra")

fviz_cluster(db, data = res$scores, stand = FALSE,
ellipse = TRUE, show.clust.cent = FALSE,
geom = "point",palette = "jco", ggtheme = theme_classic())
```

```{r}
# clookup = data.frame()
# 
# for(c in 1:length(df)){
#   clookup = rbind(clookup, c(c, row.names(df)[c], db$cluster[c]))
# }
# 
# colnames(clookup) = c('id','name', 'cluster')
```

```{r}
clust_map = read.csv("datasets/clusters.csv")
```

```{r}
dt = read.csv('datasets/matches.csv')
```

## match results


```{r}
pkmn_count = aggregate(dt$score, by=list(pokemon=dt$pokemon), FUN=length)
pkmn_count[order(-pkmn_count$x),][1:7,]
```

```{r}
shapiro.test(dt$score)
```

```{r}
library(ggpubr)
ggqqplot(dt$score)
ggdensity(dt, x = "score", fill = "lightgray", title = "score") +
  stat_overlay_normal_density(color = "red", linetype = "dashed")
```

### Forward Selection as suggested behavior

Let's see regression for two different pokemons 

```{r}
library(leaps)
cols = c('level', 'score', 'kill', 'assist', 'interrupt', 'damage_done', 'damage_taken', 'damage_healed')
```

Why Y and Log X? Interpretation purpose:

LINEAR: A 1% increase in X would lead to a β% increase/decrease in Y
LOGIT: A k-factor increase in X would lead to a k**β increase in odds.

https://stats.stackexchange.com/questions/8318/interpretation-of-log-transformed-predictors-in-logistic-regression

#### Crustle

```{r}
who = "Crustle"

dtw = dt[dt$pokemon == who, ]
dtw[,cols] = log(dtw[,cols]+1)

regfit.fwd = regsubsets(win ~ level + score + kill + assist + interrupt + damage_done + damage_taken + damage_healed, data=dtw, method="forward")

summary(regfit.fwd)
```

```{r}
glm.fit <- glm(win ~ assist + level , data = dtw, family=binomial(link='logit'))
summary(glm.fit)
```

#### Pikachu

```{r}
who = "Pikachu"

dtw = dt[dt$pokemon == who, ]
dtw[,cols] = log(dtw[,cols]+1)

regfit.fwd = regsubsets(win ~ level + score + kill + assist + interrupt + damage_done + damage_taken + damage_healed, data=dtw, method="forward")

summary(regfit.fwd)
```

```{r}
glm.fit <- glm(win ~ damage_taken + score , data = dtw, family=binomial(link='logit'))
summary(glm.fit)
```


## dumming
```{r}
dp = read.csv("datasets/pivot.group.matches.csv")
```

let's move out of linearity:

```{r}
# for( c in names(dp)[55:60]){
#   dp[paste0(c,'_2')] = as.integer(dp[c] > 1)
#   dp[c] = as.integer(dp[c] == 1)
# }

for( c in names(dp)[55:60]){
  dp[paste0(c,'_2')] = (dp[c]^2)[1]
}
```

### teammate
```{r}
names(dp)[55:60]
```

```{r}
regfit.fwd = regsubsets(win ~ support + versatile + atk_ranged + sp_atk + speedster + defence
               + support_2 + versatile_2 + atk_ranged_2 + sp_atk_2 + speedster_2 + defence_2
               , data=dp, method="exhaustive")

summary(regfit.fwd)
```


```{r}
glm.fit <- glm(win ~ support + versatile + atk_ranged + sp_atk + speedster + defence
               + support_2 + versatile_2 + atk_ranged_2 + sp_atk_2 + speedster_2 + defence_2
               , data = dp, family=binomial(link='logit'))
summary(glm.fit)
```

#### single pokemon


```{r}
glm.fit <- glm(win ~ Absol + Aegislash + Azumarill + Blastoise + Blissey + Buzzwole + Charizard + Cinderace + Cramorant + Crustle + Decidueye + Delphox + Dodrio + Dragonite + Duraludon + Eldegoss + Espeon + Garchomp + Gardevoir + Gengar + Glaceon + Greedent + Greninja + Hoopa + Lucario + Machamp + Mamoswine + Mew + MrMime + Ninetales + Pikachu + Scizor + Scyther + Slowbro + Snorlax + Sylveon + Talonflame + Trevenant + Tsareena + Tyranitar + Venusaur + Wigglytuff + Zeraora
               , data = dp, family=binomial(link='logit'))
summary(glm.fit)

```

## TREE - alberelli belli belli
```{r}
dp_group = dp[55:60]
dp_group$win = dp$win
```


```{r}
library(tree)
tree <- tree(win ~ ., data = dp_group)
summary(tree)
plot(tree)
text(tree, pretty = 0)
```

```{r}
yhat <- predict(tree, newdata = dp_group)
plot(yhat, dp_group$win)
abline(0, 1)
mean((yhat - dp_group$win)^2)
```

```{r, message=FALSE, warning=FALSE}
library("rpart")
library("rpart.plot")
tree1 <- rpart(win ~ ., data = dp_group, control = rpart.control(cp = 0, minsplit = 10, maxsurrogate = 10))
printcp(tree1)
rpart.plot(tree1)
```

```{r}
yhat <- predict(tree1, newdata = dp_group)
plot(yhat, dp_group$win)
abline(0, 1)
mean((yhat - dp_group$win)^2)
```


```{r, message=FALSE, warning=FALSE}
library("partykit")
library("party")
tree2 <- ctree(win ~ ., data = dp_group, controls = ctree_control(testtype = "Teststatistic", maxsurrogate = 1, mincriterion = 0.5, minsplit = 1))
plot(tree2, cgp = gpar(fontsize = 2))
```


```{r}
yhat <- predict(tree2, newdata = dp_group)
plot(yhat, dp_group$win)
abline(0, 1)
mean((yhat - dp_group$win)^2)
```
